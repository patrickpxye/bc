{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70d1c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# BIOSCAN\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from bioscan_dataset import BIOSCAN5M\n",
    "from bioscan_dataset.bioscan5m import RGB_MEAN, RGB_STDEV\n",
    "\n",
    "image_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        transforms.Normalize(mean=RGB_MEAN, std=RGB_STDEV),\n",
    "    ]\n",
    ")\n",
    "# Create a DNA transform, mapping from characters to integers and padding to a fixed length\n",
    "charmap = {\"P\": 0, \"A\": 1, \"C\": 2, \"G\": 3, \"T\": 4, \"N\": 5}\n",
    "dna_transform = lambda seq: torch.tensor(\n",
    "    [charmap[char] for char in seq] + [0] * (660 - len(seq)), dtype=torch.long\n",
    ")\n",
    "\n",
    "ds_train = BIOSCAN5M(\n",
    "    root=\".\",\n",
    "    split=\"val\",\n",
    "    transform=image_transform,\n",
    "    dna_transform=dna_transform,\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a018108f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# WIT\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_mutual_info_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sacrebleu.metrics import CHRF\n",
    "from k_means_constrained import KMeansConstrained\n",
    "\n",
    "n_samples = 100000\n",
    "\n",
    "# 1. Stream the WIT dataset (avoid full download)\n",
    "ds_stream = load_dataset(\"wikimedia/wit_base\", split=\"train\", streaming=True)\n",
    "\n",
    "# 2. Take first `n_samples` examples from the stream\n",
    "subset = list(itertools.islice(ds_stream, n_samples))\n",
    "\n",
    "def extract_english_caption(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the substring following 'English:' up to the next '<Lang>:' or end of string.\n",
    "    Returns an empty string if no English part is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'English:\\s*(.*?)(?=\\s*[A-Z][^:]*:|$)', s)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "\n",
    "# 3. Build DataFrame with raw embedding (\"x\"), caption (\"y\"), and raw features (\"raw_f\")\n",
    "df = pd.DataFrame({\n",
    "    \"x\": [item.get(\"embedding\") for item in subset],\n",
    "    \"y\": [item.get(\"caption_attribution_description\") for item in subset],\n",
    "    \"raw_f\": [item.get(\"wit_features\") for item in subset]\n",
    "})\n",
    "\n",
    "# 4. Drop rows with missing x or y, and clean y\n",
    "df = df.dropna(subset=[\"x\", \"y\"]).reset_index(drop=True)\n",
    "df[\"y\"] = df[\"y\"].apply(extract_english_caption)\n",
    "df = df[df[\"y\"].astype(bool)].reset_index(drop=True)\n",
    "\n",
    "# 5. Extract only the English page_title into \"z\"\n",
    "def extract_en_title(wf):\n",
    "    langs  = wf.get(\"language\", [])\n",
    "    titles = wf.get(\"page_title\", [])\n",
    "    if \"en\" in langs:\n",
    "        return titles[langs.index(\"en\")]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df[\"z\"] = df[\"raw_f\"].apply(extract_en_title)\n",
    "df = df.dropna(subset=[\"z\"]).reset_index(drop=True)\n",
    "\n",
    "# 6. Encode all captions with Sentence-BERT into \"yv\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dim encoder\n",
    "df[\"yv\"] = model.encode(\n",
    "    df[\"y\"].tolist(),\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ").tolist()\n",
    "\n",
    "# 7. Encode English titles (\"z\") into \"zw\"\n",
    "df[\"zv\"] = model.encode(\n",
    "    df[\"z\"].tolist(),\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c49d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Flickr30k\n",
    "\n",
    "import io\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torchvision.models import (\n",
    "    resnet50, ResNet50_Weights,\n",
    "    vit_b_16, ViT_B_16_Weights,\n",
    "    efficientnet_b0, EfficientNet_B0_Weights\n",
    ")\n",
    "\n",
    "# 1) Load only the split\n",
    "ds = load_dataset(\"nlphuji/flickr30k\", split=\"test\")\n",
    "\n",
    "# 2) Prepare models & transforms\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#img_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2).to(device).eval()\n",
    "\n",
    "img_model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1).to(device)\n",
    "\n",
    "feature_extractor = torch.nn.Sequential(*list(img_model.children())[:-1])\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# 3) Batch‐encode function, handling both PIL images and raw bytes\n",
    "def encode_batch(batch):\n",
    "    imgs = []\n",
    "    for img in batch[\"image\"]:\n",
    "        if isinstance(img, bytes):\n",
    "            img = Image.open(io.BytesIO(img))\n",
    "        # if it’s already a PIL image, skip the BytesIO step\n",
    "        imgs.append(img.convert(\"RGB\"))\n",
    "    tensor_batch = torch.stack([preprocess(img) for img in imgs]).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = feature_extractor(tensor_batch).view(len(imgs), -1)\n",
    "    batch[\"image_emb\"] = feats.cpu().numpy()\n",
    "\n",
    "    # captions → list of 5×D arrays\n",
    "    batch[\"caption_embs\"] = [\n",
    "        sbert.encode(caps, convert_to_numpy=True)\n",
    "        for caps in batch[\"caption\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "ds = ds.map(\n",
    "    encode_batch,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=[\"image\"],\n",
    ")\n",
    "\n",
    "ds.to_parquet(\"flickr30k_eff.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442d1fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Coco\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "# Settings\n",
    "dataType    = 'val2017'\n",
    "imgDir      = 'val2017'  # adjust to your local image folder\n",
    "capAnnFile  = f'annotations/captions_{dataType}.json'\n",
    "instAnnFile = f'annotations/instances_{dataType}.json'\n",
    "output_csv  = 'coco_2.csv'\n",
    "\n",
    "# Load COCO APIs\n",
    "coco_caps = COCO(capAnnFile)\n",
    "coco_inst = COCO(instAnnFile)\n",
    "\n",
    "# Collect categories with >=100 simple-image candidates\n",
    "all_cats = coco_inst.loadCats(coco_inst.getCatIds())\n",
    "random.seed(42)\n",
    "random.shuffle(all_cats)\n",
    "\n",
    "selected_cats = []\n",
    "for cat in all_cats:\n",
    "    if len(selected_cats) == 45:\n",
    "        break\n",
    "    cat_id   = cat['id']\n",
    "    cat_name = cat['name']\n",
    "    img_ids  = coco_inst.getImgIds(catIds=[cat_id])\n",
    "\n",
    "    # filter simple-category images\n",
    "    valid_img_ids = []\n",
    "    for img_id in img_ids:\n",
    "        inst_anns = coco_inst.loadAnns(coco_inst.getAnnIds(imgIds=[img_id]))\n",
    "        cats_in_img = {ann['category_id'] for ann in inst_anns}\n",
    "        if cats_in_img == {cat_id}:\n",
    "            valid_img_ids.append(img_id)\n",
    "\n",
    "    if len(valid_img_ids) >= 40:\n",
    "        selected_cats.append({\n",
    "            'id': cat_id,\n",
    "            'name': cat_name,\n",
    "            'img_ids': valid_img_ids\n",
    "        })\n",
    "\n",
    "# Prepare models & transforms\n",
    "device    = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "resnet    = models.resnet50(pretrained=True)\n",
    "resnet.fc = torch.nn.Identity()\n",
    "resnet    = resnet.to(device).eval()\n",
    "transform = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "sbert = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Build records\n",
    "records = []\n",
    "for cat in selected_cats:\n",
    "    cat_name      = cat['name']\n",
    "    valid_img_ids = cat['img_ids']\n",
    "\n",
    "    # Collect all caption-rows for this category\n",
    "    cat_rows = []\n",
    "    for img_id in valid_img_ids:\n",
    "        # encode image\n",
    "        img_path = os.path.join(imgDir, f\"{img_id:012d}.jpg\")\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        with torch.no_grad():\n",
    "            x_t    = transform(img).unsqueeze(0).to(device)\n",
    "            x_feat = resnet(x_t).squeeze(0).cpu().tolist()\n",
    "\n",
    "        # explode by caption\n",
    "        cap_anns = coco_caps.loadAnns(coco_caps.getAnnIds(imgIds=[img_id]))\n",
    "        for ann in cap_anns:\n",
    "            caption = ann['caption']\n",
    "            yv = sbert.encode(caption).tolist()\n",
    "            cat_rows.append({\n",
    "                'cat': cat_name,\n",
    "                'image_id': img_id,\n",
    "                'x'  : json.dumps(x_feat),\n",
    "                'y'  : caption,\n",
    "                'yv' : json.dumps(yv),\n",
    "            })\n",
    "\n",
    "    # Prune/exactly 100 samples per category\n",
    "    sampled_rows = random.sample(cat_rows, 200)\n",
    "    records.extend(sampled_rows)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Saved {len(df)} rows to {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
